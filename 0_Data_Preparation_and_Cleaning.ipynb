{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cdc43a1",
   "metadata": {},
   "source": [
    "# Data Preparation & Cleaning\n",
    "\n",
    "The raw job-posting dataset is prepared into a clean file that is used in all downstream analysis notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092cb5c",
   "metadata": {},
   "source": [
    "## Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c958786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "df = pd.read_csv('E:/Projects/Python_Data_Project/job_postings_flat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8faa93a",
   "metadata": {},
   "source": [
    "## Raw Data Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f4e1d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title_short</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_via</th>\n",
       "      <th>job_schedule_type</th>\n",
       "      <th>job_work_from_home</th>\n",
       "      <th>search_location</th>\n",
       "      <th>job_posted_date</th>\n",
       "      <th>job_no_degree_mention</th>\n",
       "      <th>job_health_insurance</th>\n",
       "      <th>job_country</th>\n",
       "      <th>salary_rate</th>\n",
       "      <th>salary_year_avg</th>\n",
       "      <th>salary_hour_avg</th>\n",
       "      <th>company_name</th>\n",
       "      <th>job_skills</th>\n",
       "      <th>job_type_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>via CareerBuilder</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>New York, United States</td>\n",
       "      <td>2023-01-01 00:00:04</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Metasys Technologies</td>\n",
       "      <td>['sql', 'snowflake', 'visio', 'jira', 'conflue...</td>\n",
       "      <td>{'analyst_tools': ['visio'], 'async': ['jira',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>via CareerBuilder</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>New York, United States</td>\n",
       "      <td>2023-01-01 00:00:22</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guidehouse</td>\n",
       "      <td>['sql', 'python', 'r', 'azure', 'snowflake', '...</td>\n",
       "      <td>{'analyst_tools': ['tableau', 'excel'], 'cloud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Fairfax, VA</td>\n",
       "      <td>via CareerBuilder</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>New York, United States</td>\n",
       "      <td>2023-01-01 00:00:24</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Protask</td>\n",
       "      <td>['sql', 'jira']</td>\n",
       "      <td>{'async': ['jira'], 'programming': ['sql']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Senior Data Analyst / Platform Experience</td>\n",
       "      <td>Worcester, MA</td>\n",
       "      <td>via LinkedIn</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>New York, United States</td>\n",
       "      <td>2023-01-01 00:00:27</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atria Wealth Solutions</td>\n",
       "      <td>['sql', 'atlassian', 'jira']</td>\n",
       "      <td>{'async': ['jira'], 'other': ['atlassian'], 'p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Jr. Data Analyst</td>\n",
       "      <td>Torrance, CA</td>\n",
       "      <td>via Recruit.net</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>False</td>\n",
       "      <td>California, United States</td>\n",
       "      <td>2023-01-01 00:00:38</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aquent</td>\n",
       "      <td>['excel']</td>\n",
       "      <td>{'analyst_tools': ['excel']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       job_title_short                                  job_title  \\\n",
       "0         Data Analyst                               Data Analyst   \n",
       "1         Data Analyst                               Data Analyst   \n",
       "2         Data Analyst                               Data Analyst   \n",
       "3  Senior Data Analyst  Senior Data Analyst / Platform Experience   \n",
       "4         Data Analyst                           Jr. Data Analyst   \n",
       "\n",
       "     job_location            job_via job_schedule_type  job_work_from_home  \\\n",
       "0    New York, NY  via CareerBuilder         Full-time               False   \n",
       "1  Washington, DC  via CareerBuilder         Full-time               False   \n",
       "2     Fairfax, VA  via CareerBuilder         Full-time               False   \n",
       "3   Worcester, MA       via LinkedIn         Full-time               False   \n",
       "4    Torrance, CA    via Recruit.net         Full-time               False   \n",
       "\n",
       "             search_location      job_posted_date  job_no_degree_mention  \\\n",
       "0    New York, United States  2023-01-01 00:00:04                  False   \n",
       "1    New York, United States  2023-01-01 00:00:22                  False   \n",
       "2    New York, United States  2023-01-01 00:00:24                  False   \n",
       "3    New York, United States  2023-01-01 00:00:27                  False   \n",
       "4  California, United States  2023-01-01 00:00:38                  False   \n",
       "\n",
       "   job_health_insurance    job_country salary_rate  salary_year_avg  \\\n",
       "0                 False  United States         NaN              NaN   \n",
       "1                  True  United States         NaN              NaN   \n",
       "2                 False  United States         NaN              NaN   \n",
       "3                  True  United States         NaN              NaN   \n",
       "4                 False  United States         NaN              NaN   \n",
       "\n",
       "   salary_hour_avg            company_name  \\\n",
       "0              NaN    Metasys Technologies   \n",
       "1              NaN              Guidehouse   \n",
       "2              NaN                 Protask   \n",
       "3              NaN  Atria Wealth Solutions   \n",
       "4              NaN                  Aquent   \n",
       "\n",
       "                                          job_skills  \\\n",
       "0  ['sql', 'snowflake', 'visio', 'jira', 'conflue...   \n",
       "1  ['sql', 'python', 'r', 'azure', 'snowflake', '...   \n",
       "2                                    ['sql', 'jira']   \n",
       "3                       ['sql', 'atlassian', 'jira']   \n",
       "4                                          ['excel']   \n",
       "\n",
       "                                     job_type_skills  \n",
       "0  {'analyst_tools': ['visio'], 'async': ['jira',...  \n",
       "1  {'analyst_tools': ['tableau', 'excel'], 'cloud...  \n",
       "2        {'async': ['jira'], 'programming': ['sql']}  \n",
       "3  {'async': ['jira'], 'other': ['atlassian'], 'p...  \n",
       "4                       {'analyst_tools': ['excel']}  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c0b2038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1780669 entries, 0 to 1780668\n",
      "Data columns (total 17 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   job_title_short        object \n",
      " 1   job_title              object \n",
      " 2   job_location           object \n",
      " 3   job_via                object \n",
      " 4   job_schedule_type      object \n",
      " 5   job_work_from_home     bool   \n",
      " 6   search_location        object \n",
      " 7   job_posted_date        object \n",
      " 8   job_no_degree_mention  bool   \n",
      " 9   job_health_insurance   bool   \n",
      " 10  job_country            object \n",
      " 11  salary_rate            object \n",
      " 12  salary_year_avg        float64\n",
      " 13  salary_hour_avg        float64\n",
      " 14  company_name           object \n",
      " 15  job_skills             object \n",
      " 16  job_type_skills        object \n",
      "dtypes: bool(3), float64(2), object(12)\n",
      "memory usage: 195.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da52b0ac",
   "metadata": {},
   "source": [
    "## Create Working Copy\n",
    "\n",
    "Create a separate working dataframe so all changes are applied in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "285863e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = df.copy()\n",
    "df_work = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c343d811",
   "metadata": {},
   "source": [
    "## Basic Text Standardization\n",
    "\n",
    "Standardize key text fields to reduce inconsistencies caused by formatting differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f05a797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT STANDARDIZATION – VALUE CHANGES\n",
      "job_title: 8917 values changed\n",
      "company_name: 1700 values changed\n",
      "search_location: 0 values changed\n",
      "job_location: 4354 values changed\n",
      "job_schedule_type: 0 values changed\n",
      "job_country: 0 values changed\n",
      "job_title_short: 0 values changed\n",
      "job_via: 0 values changed\n"
     ]
    }
   ],
   "source": [
    "def clean_text_series(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(\"string\")\n",
    "         .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "         .str.strip()\n",
    "    )\n",
    "\n",
    "text_cols = [\n",
    "    \"job_title\",\n",
    "    \"company_name\",\n",
    "    \"search_location\",\n",
    "    \"job_location\",\n",
    "    \"job_schedule_type\",\n",
    "    \"job_country\",\n",
    "    \"job_title_short\",\n",
    "    \"job_via\",\n",
    "]\n",
    "\n",
    "df_before = df_work.copy()\n",
    "\n",
    "for col in text_cols:\n",
    "    df_work[col] = clean_text_series(df_work[col])\n",
    "\n",
    "print(\"\\nTEXT STANDARDIZATION – VALUE CHANGES\")\n",
    "for col in text_cols:\n",
    "    changed = (df_before[col] != df_work[col]) & ~(df_before[col].isna() & df_work[col].isna())\n",
    "    print(f\"{col}: {changed.sum()} values changed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05277ac8",
   "metadata": {},
   "source": [
    "## Dates and Weekly Bucket\n",
    "\n",
    "Convert posting dates to datetime and create a weekly bucket used for time grouping and deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ad27f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work[\"job_posted_date\"] = pd.to_datetime(df_work[\"job_posted_date\"], errors=\"coerce\")\n",
    "df_work[\"posted_week\"] = df_work[\"job_posted_date\"].dt.to_period(\"W\").astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6f44d9",
   "metadata": {},
   "source": [
    "## Job Posting Platform\n",
    "\n",
    "Clean and normalize job source labels to make grouping by source more consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "634fa63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work[\"job_via\"] = (\n",
    "    df_work[\"job_via\"]\n",
    "        .str.replace(r\"^via\\s+\", \"\", regex=True, case=False)\n",
    "        .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5ef6b",
   "metadata": {},
   "source": [
    "## Company name standardization\n",
    "\n",
    "Standardize company names to reduce duplicated labels caused by minor variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30a77070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_work[\"company_name\"] = df_work[\"company_name\"].fillna(\"Unknown\").astype(str).str.strip()\n",
    "df_work[\"company_name\"] = df_work[\"company_name\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "s = df_work[\"company_name\"].str.lower()\n",
    "\n",
    "df_work[\"company_name\"] = np.where(s.str.contains(\"bebee\"), \"BeBee\", df_work[\"company_name\"])\n",
    "df_work[\"company_name\"] = np.where(s.str.contains(\"jobs via dice\") | s.eq(\"dice\"), \"Dice\", df_work[\"company_name\"])\n",
    "df_work[\"company_name\"] = np.where(s.str.contains(\"confidential\"), \"Confidential\", df_work[\"company_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c72ae13",
   "metadata": {},
   "source": [
    "## Week-Based Deduplication\n",
    "\n",
    "Remove repost-style duplicates within the same week using a composite deduplication key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68c3f427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WEEK-BASED DEDUPLICATION ===\n",
      "Rows before: 1,780,669\n",
      "Rows after:  1,550,333\n",
      "Δ rows:      -230,336\n",
      "Removed (%): 12.94%\n"
     ]
    }
   ],
   "source": [
    "def report_step(step_name: str, before_rows: int, after_rows: int) -> None:\n",
    "    delta = after_rows - before_rows\n",
    "    removed_pct = (1 - after_rows / before_rows) * 100 if before_rows else 0\n",
    "    print(f\"\\n=== {step_name} ===\")\n",
    "    print(f\"Rows before: {before_rows:,}\")\n",
    "    print(f\"Rows after:  {after_rows:,}\")\n",
    "    print(f\"Δ rows:      {delta:,}\")\n",
    "    print(f\"Removed (%): {removed_pct:.2f}%\")\n",
    "\n",
    "dedup_cols = [\n",
    "    \"job_title\",\n",
    "    \"company_name\",\n",
    "    \"search_location\",\n",
    "    \"job_schedule_type\",\n",
    "    \"job_work_from_home\",\n",
    "    \"posted_week\",\n",
    "]\n",
    "\n",
    "df_cleaned = (\n",
    "    df_work\n",
    "        .sort_values(\"job_posted_date\")\n",
    "        .drop_duplicates(subset=dedup_cols, keep=\"last\")\n",
    "        .copy()\n",
    ")\n",
    "\n",
    "report_step(\"WEEK-BASED DEDUPLICATION\", len(df_work), len(df_cleaned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f73d38",
   "metadata": {},
   "source": [
    "## Deduplicate Skills Within a Posting\n",
    "\n",
    "- Remove repeated skills within each posting so later explode()-based analyses do not overcount skill mentions.\n",
    "\n",
    "- Parse job_skills from its raw CSV string format (stringified Python list) into an actual Python list using ast.literal_eval, matching the parsing logic used in downstream notebooks.\n",
    "\n",
    "- Deduplicate each skills list while preserving the original order of first occurrence (keeps the “first-seen” skill sequence intact).\n",
    "\n",
    "- Report the duplicate-skill posting rate before and after deduplication, computed only for rows where job_skills is a non-empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fa70e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate-skill postings BEFORE dedupe: 8.49%\n",
      "Duplicate-skill postings AFTER  dedupe: 0.00%\n"
     ]
    }
   ],
   "source": [
    "df_cleaned[\"job_skills\"] = df_cleaned[\"job_skills\"].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "def dup_rate(series: pd.Series) -> float:\n",
    "    mask = series.apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "    return series[mask].apply(lambda lst: len(lst) != len(set(lst))).mean()\n",
    "\n",
    "before = dup_rate(df_cleaned[\"job_skills\"])\n",
    "\n",
    "df_cleaned[\"job_skills\"] = df_cleaned[\"job_skills\"].apply(\n",
    "    lambda lst: list(dict.fromkeys(lst)) if isinstance(lst, list) else lst\n",
    ")\n",
    "\n",
    "after = dup_rate(df_cleaned[\"job_skills\"])\n",
    "\n",
    "print(f\"Duplicate-skill postings BEFORE dedupe: {before:.2%}\")\n",
    "print(f\"Duplicate-skill postings AFTER  dedupe: {after:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ff429",
   "metadata": {},
   "source": [
    "## Drop Helper Columns\n",
    "\n",
    "Drop intermediate helper columns that are no longer needed after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe88093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.drop(columns=[\"posted_week\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda261a",
   "metadata": {},
   "source": [
    "## Export Clean Dataset\n",
    "\n",
    "- Export the cleaned dataset to a single file used across the project.\n",
    "\n",
    "- **Key normalizations**: job_posted_date → datetime, job_via cleaned (removed “via …”), company_name standardized.\n",
    "\n",
    "- **Deduplication**: week-based dedupe on job_title + company_name + search_location + schedule + WFH + posted_week.\n",
    "\n",
    "- **Skills**: duplicates inside a single posting removed; stored in CSV as stringified lists (parsed later with ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82840a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL DATASET SUMMARY\n",
      "Raw rows: 1780669\n",
      "Final rows: 1550333\n",
      "Total rows removed: 230336\n",
      "Saved: job_postings_flat_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFINAL DATASET SUMMARY\")\n",
    "print(\"Raw rows:\", len(df_raw))\n",
    "print(\"Final rows:\", len(df_cleaned))\n",
    "print(\"Total rows removed:\", len(df_raw) - len(df_cleaned))\n",
    "\n",
    "df_cleaned.to_csv(\"job_postings_flat_cleaned.csv\", index=False)\n",
    "print(\"Saved: job_postings_flat_cleaned.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
